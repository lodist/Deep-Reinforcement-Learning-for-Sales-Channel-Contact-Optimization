{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomerInteractionEnv(gym.Env):\n",
    "    def __init__(self, dataset, total_timesteps):\n",
    "        super(CustomerInteractionEnv, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.current_step = 0\n",
    "        self.total_reward = 0  # Track cumulative reward\n",
    "        self.total_timesteps = total_timesteps  # Total timesteps for training\n",
    "        self.actions = dataset['Action'].unique().tolist() + [\"No Action\"]\n",
    "        self.action_space = spaces.Discrete(len(self.actions))\n",
    "        state_size = len(dataset.iloc[0]['State'])\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(state_size,), dtype=np.float32)\n",
    "        self.visited_states = defaultdict(int)  # Track visited states\n",
    "        self.action_counts = defaultdict(int)  # Track action frequencies\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        \"\"\"Convert the state to a NumPy array.\"\"\"\n",
    "        return np.array(state, dtype=np.float32)\n",
    "\n",
    "    def calculate_similarity(self, action1, action2):\n",
    "        \"\"\"Custom similarity measure between actions.\"\"\"\n",
    "        return 1 if action1 == action2 else 0  # Replace with actual similarity logic\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset the environment to the first step.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        self.total_reward = 0  # Reset cumulative reward\n",
    "        self.visited_states.clear()  # Reset visit counts\n",
    "        self.action_counts.clear()  # Reset action frequencies\n",
    "        initial_state = self.preprocess_state(self.dataset.iloc[self.current_step]['State'])\n",
    "        return initial_state, {}  # Return a tuple (obs, info)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return the next state, reward, terminated, truncated, and info.\"\"\"\n",
    "        row = self.dataset.iloc[self.current_step]\n",
    "        chosen_action = self.actions[action]\n",
    "\n",
    "        # Increment action count\n",
    "        self.action_counts[chosen_action] += 1\n",
    "\n",
    "        if chosen_action == \"No Action\":\n",
    "            reward = -0.01  \n",
    "        elif chosen_action == row['Action']:\n",
    "            reward = row['Reward']  # Positive reward\n",
    "            \n",
    "            # Enhance positive rewards between 10 and 20\n",
    "            if 10 <= reward <= 20:\n",
    "                reward = reward ** 1.5\n",
    "        else:\n",
    "            similarity_score = self.calculate_similarity(chosen_action, row['Action'])\n",
    "            reward = similarity_score * 0.5 - 0.2\n",
    "\n",
    "        # Penalize repeated actions\n",
    "        action_penalty = 0.001 * (self.action_counts[chosen_action] - 1)\n",
    "        reward -= action_penalty\n",
    "\n",
    "        # Exploration bonus for visiting less-explored states\n",
    "        state_tuple = tuple(self.preprocess_state(row['State']))\n",
    "        self.visited_states[state_tuple] += 1\n",
    "        decay_factor = 1 - (self.current_step / self.total_timesteps)  # Linearly decay over time\n",
    "        decay_factor = max(decay_factor, 0)  # Ensure the decay factor never goes below 0\n",
    "        exploration_bonus = (0.8 / np.sqrt(self.visited_states[state_tuple])) * decay_factor\n",
    "        exploration_bonus = min(exploration_bonus, 1.0)  # Cap the bonus at 1.0\n",
    "        reward += exploration_bonus\n",
    "\n",
    "        # Update total reward\n",
    "        self.total_reward += reward\n",
    "\n",
    "        if self.current_step % 10_000 == 0:\n",
    "            print(\n",
    "                f\"Step: {self.current_step}, Action: {chosen_action}, Reward: {reward:.2f}, \"\n",
    "                f\"Exploration Bonus: {exploration_bonus:.2f}, Total Reward: {self.total_reward:.2f}\"\n",
    "            )\n",
    "\n",
    "        self.current_step += 1\n",
    "        terminated = self.current_step >= len(self.dataset)\n",
    "        truncated = False\n",
    "        if not terminated:\n",
    "            next_state = self.preprocess_state(self.dataset.iloc[self.current_step]['State'])\n",
    "        else:\n",
    "            next_state = None\n",
    "        return next_state, reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Render the environment (optional).\"\"\"\n",
    "        print(f\"Step: {self.current_step}, Total Steps: {len(self.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENVIRONMENT INITIALIZATION \n",
    "\n",
    "# Load the RL training dataset\n",
    "rl_data = pd.read_csv(\"DRL_Training_Dataset.csv\")\n",
    "\n",
    "# Convert the State column to a list of floats\n",
    "rl_data['State'] = rl_data['State'].apply(eval)\n",
    "\n",
    "# Initialize the environment with the dataset and total timesteps\n",
    "env = CustomerInteractionEnv(dataset=rl_data, total_timesteps=2_000_000)\n",
    "\n",
    "# Check the custom environment\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "check_env(env, warn=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a DQN model\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import time\n",
    "\n",
    "\n",
    "class ProgressBarCallback(BaseCallback):\n",
    "    def __init__(self, total_timesteps, verbose=0):\n",
    "        super(ProgressBarCallback, self).__init__(verbose)\n",
    "        self.total_timesteps = total_timesteps\n",
    "        self.last_time_called = time.time()  # Initialize the timer\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        current_time = time.time()\n",
    "        # Check if one minute has passed\n",
    "        if current_time - self.last_time_called >= 180:  # 180 seconds\n",
    "            self.last_time_called = current_time\n",
    "            # Calculate progress\n",
    "            current_progress = self.num_timesteps / self.total_timesteps * 100\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Training progress: {current_progress:.2f}%\")\n",
    "        return True\n",
    "\n",
    "\n",
    "# Hyperparameters hidden in a dictionary\n",
    "drl_hyperparameters = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"gamma\": 0.98,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"buffer_size\": 300_000,\n",
    "    \"learning_starts\": 5_000,\n",
    "    \"batch_size\": 128,\n",
    "    \"tau\": 0.01,\n",
    "    \"train_freq\": 4,\n",
    "    \"exploration_fraction\": 0.5,\n",
    "    \"exploration_final_eps\": 0.02,\n",
    "    \"target_update_interval\": 5_000,\n",
    "    \"max_grad_norm\": 5,\n",
    "    \"policy_kwargs\": {\"net_arch\": [512, 256]},\n",
    "    \"verbose\": 1,\n",
    "    \"total_timesteps\": 5_000_000,\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize the DQN model\n",
    "model = DQN(\n",
    "    drl_hyperparameters[\"policy\"],\n",
    "    env,\n",
    "    gamma=drl_hyperparameters[\"gamma\"],\n",
    "    learning_rate=drl_hyperparameters[\"learning_rate\"],\n",
    "    buffer_size=drl_hyperparameters[\"buffer_size\"],\n",
    "    learning_starts=drl_hyperparameters[\"learning_starts\"],\n",
    "    batch_size=drl_hyperparameters[\"batch_size\"],\n",
    "    tau=drl_hyperparameters[\"tau\"],\n",
    "    train_freq=drl_hyperparameters[\"train_freq\"],\n",
    "    exploration_fraction=drl_hyperparameters[\"exploration_fraction\"],\n",
    "    exploration_final_eps=drl_hyperparameters[\"exploration_final_eps\"],\n",
    "    target_update_interval=drl_hyperparameters[\"target_update_interval\"],\n",
    "    max_grad_norm=drl_hyperparameters[\"max_grad_norm\"],\n",
    "    policy_kwargs=drl_hyperparameters[\"policy_kwargs\"],\n",
    "    verbose=drl_hyperparameters[\"verbose\"],\n",
    ")\n",
    "\n",
    "# Train the model with a progress bar callback\n",
    "progress_bar = ProgressBarCallback(\n",
    "    total_timesteps=drl_hyperparameters[\"total_timesteps\"], verbose=1\n",
    ")\n",
    "model.learn(\n",
    "    total_timesteps=drl_hyperparameters[\"total_timesteps\"], callback=progress_bar\n",
    ")\n",
    "\n",
    "model.save(\"dqn_customer_interaction_V5\")\n",
    "print(\"Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL EVALUATION\n",
    "\n",
    "# Load the model\n",
    "model = DQN.load(\"dqn_customer_interaction\")\n",
    "\n",
    "def evaluate_model_performance(env, model, dataset, max_steps=1000):\n",
    "    \"\"\"Evaluate model vs dataset, tracking rewards, actions, total contacts, and cost efficiency.\"\"\"\n",
    "    obs = env.reset()[0]\n",
    "    model_total_reward = 0\n",
    "    dataset_total_reward = 0\n",
    "    steps = 0\n",
    "    model_path = []\n",
    "    dataset_path = []\n",
    "    \n",
    "    no_action_count = 0\n",
    "    dm_count = 0  # Count for \"DM_sent\" in model\n",
    "    em_count = 0  # Count for \"EMsent\" in model\n",
    "    dataset_dm_count = 0  # Count for \"DM_sent\" in dataset\n",
    "    dataset_em_count = 0  # Count for \"EMsent\" in dataset\n",
    "\n",
    "    while steps < max_steps:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        dataset_action = env.actions.index(dataset.iloc[steps]['Action'])\n",
    "\n",
    "        # Step model\n",
    "        obs, model_reward, terminated, truncated, _ = env.step(action)\n",
    "        model_action = env.actions[action]\n",
    "        model_path.append({\"step\": steps, \"state\": obs.tolist(), \"action\": model_action, \"reward\": model_reward})\n",
    "        model_total_reward += model_reward\n",
    "\n",
    "        # Count actions\n",
    "        if model_action == \"No Action\":\n",
    "            no_action_count += 1\n",
    "        elif model_action == \"DM_sent\":\n",
    "            dm_count += 1\n",
    "        elif model_action == \"EMsent\":\n",
    "            em_count += 1\n",
    "\n",
    "        # Step dataset (simulate dataset reward for same state and dataset action)\n",
    "        env.current_step = steps\n",
    "        _, dataset_reward, _, _, _ = env.step(dataset_action)\n",
    "        dataset_action_label = env.actions[dataset_action]\n",
    "        dataset_path.append({\"step\": steps, \"state\": obs.tolist(), \"action\": dataset_action_label, \"reward\": dataset_reward})\n",
    "        dataset_total_reward += dataset_reward\n",
    "\n",
    "        # Count dataset actions\n",
    "        if dataset_action_label == \"DM_sent\":\n",
    "            dataset_dm_count += 1\n",
    "        elif dataset_action_label == \"EMsent\":\n",
    "            dataset_em_count += 1\n",
    "\n",
    "        steps += 1\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # Calculate total contacts\n",
    "    model_total_contacts = dm_count + em_count\n",
    "    dataset_total_contacts = dataset_dm_count + dataset_em_count\n",
    "\n",
    "    # Define Cost (DMs cost 1 CHF each)\n",
    "    model_total_cost = dm_count * 1  # 1 CHF per DM_sent\n",
    "    dataset_total_cost = dataset_dm_count * 1  # 1 CHF per DM_sent in dataset\n",
    "\n",
    "    # Calculate reward per contact\n",
    "    model_reward_per_contact = model_total_reward / model_total_contacts if model_total_contacts > 0 else 0\n",
    "    dataset_reward_per_contact = dataset_total_reward / dataset_total_contacts if dataset_total_contacts > 0 else 0\n",
    "\n",
    "    # Calculate cost per reward point\n",
    "    model_cost_per_reward = model_total_cost / model_total_reward if model_total_reward != 0 else float('inf')\n",
    "    dataset_cost_per_reward = dataset_total_cost / dataset_total_reward if dataset_total_reward != 0 else float('inf')\n",
    "\n",
    "    # Print Results\n",
    "    print(f\"Model Total Reward: {model_total_reward:.2f}\")\n",
    "    print(f\"Dataset Total Reward: {dataset_total_reward:.2f}\")\n",
    "    print(f\"Number of 'No Action' Decisions: {no_action_count} out of {max_steps}\")\n",
    "    print(f\"Number of 'DM_Sent' Decisions in Model: {dm_count} out of {max_steps}\")\n",
    "    print(f\"Number of 'DM_Sent' Decisions in Dataset: {dataset_dm_count} out of {max_steps}\")\n",
    "    print(f\"Number of 'EM_Sent' Decisions in Model: {em_count} out of {max_steps}\")\n",
    "    print(f\"Number of 'EM_Sent' Decisions in Dataset: {dataset_em_count} out of {max_steps}\")\n",
    "    print(f\"Total Contacts in Model: {model_total_contacts} out of {max_steps}\")\n",
    "    print(f\"Total Contacts in Dataset: {dataset_total_contacts} out of {max_steps}\")\n",
    "    print(f\"Difference in Contacts Between Model and Dataset: {model_total_contacts - dataset_total_contacts}\")\n",
    "    print(f\"Model Total Cost: {model_total_cost} CHF\")\n",
    "    print(f\"Dataset Total Cost: {dataset_total_cost} CHF\")\n",
    "    print(f\"Model Reward per Contact: {model_reward_per_contact:.4f}\")\n",
    "    print(f\"Dataset Reward per Contact: {dataset_reward_per_contact:.4f}\")\n",
    "    print(f\"Model Cost per Reward Point: {model_cost_per_reward:.4f} CHF\")\n",
    "    print(f\"Dataset Cost per Reward Point: {dataset_cost_per_reward:.4f} CHF\")\n",
    "\n",
    "    return model_path, dataset_path\n",
    "\n",
    "\n",
    "def plot_cumulative_rewards_comparison(model_path, dataset_path):\n",
    "    \"\"\"Plot cumulative reward comparison.\"\"\"\n",
    "    model_rewards = [step[\"reward\"] for step in model_path]\n",
    "    dataset_rewards = [step[\"reward\"] for step in dataset_path]\n",
    "\n",
    "    model_cumulative = np.cumsum(model_rewards)\n",
    "    dataset_cumulative = np.cumsum(dataset_rewards)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(model_cumulative, label=\"Model Cumulative Reward\", color=\"purple\")\n",
    "    plt.plot(dataset_cumulative, label=\"Dataset Cumulative Reward (Baseline)\", color=\"grey\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    plt.title(\"Cumulative Reward Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def plot_action_distribution_comparison(model_path, dataset_path):\n",
    "    \"\"\"Plot action distribution for model vs dataset.\"\"\"\n",
    "    model_actions = [step[\"action\"] for step in model_path]\n",
    "    dataset_actions = [step[\"action\"] for step in dataset_path]\n",
    "\n",
    "    model_action_counts = Counter(model_actions)\n",
    "    dataset_action_counts = Counter(dataset_actions)\n",
    "\n",
    "    actions = sorted(set(model_action_counts.keys()).union(dataset_action_counts.keys()))\n",
    "\n",
    "    model_counts = [model_action_counts.get(action, 0) for action in actions]\n",
    "    dataset_counts = [dataset_action_counts.get(action, 0) for action in actions]\n",
    "\n",
    "    x = np.arange(len(actions))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(x - 0.2, model_counts, width=0.4, label=\"Model Actions\", color=\"purple\")\n",
    "    plt.bar(x + 0.2, dataset_counts, width=0.4, label=\"Dataset Actions\", color=\"grey\")\n",
    "    plt.xticks(x, actions, rotation=45)\n",
    "    plt.xlabel(\"Actions\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Action Distribution Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Sample dataset for evaluation\n",
    "eval_subset = rl_data.sample(10000, random_state=100).reset_index(drop=True)\n",
    "eval_env = CustomerInteractionEnv(eval_subset, total_timesteps=10_000_000)\n",
    "\n",
    "# Run evaluation\n",
    "model_path, dataset_path = evaluate_model_performance(eval_env, model, eval_subset, max_steps=1000)\n",
    "\n",
    "# Display results\n",
    "plot_cumulative_rewards_comparison(model_path, dataset_path)\n",
    "plot_action_distribution_comparison(model_path, dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL APPLICATION TEST\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"dqn_customer_interaction\"\n",
    "model = DQN.load(model_path)\n",
    "\n",
    "# Load the test dataset\n",
    "rl_data_test = pd.read_csv(\"DRL_Training_Dataset.csv\")\n",
    "\n",
    "# Extract the first state from the dataset\n",
    "state = rl_data_test['State'].iloc[0]\n",
    "\n",
    "if isinstance(state, str):\n",
    "    state = eval(state)\n",
    "\n",
    "# Convert the state to a NumPy array\n",
    "state = np.array(state, dtype=np.float32)\n",
    "\n",
    "# Get the predicted action from the model\n",
    "action, _ = model.predict(state, deterministic=True)\n",
    "\n",
    "print(f\"Predicted action index: {action}\")\n",
    "\n",
    "# Assuming the environment actions list is available:\n",
    "# (Replace this with the actual initialization logic for your environment's actions)\n",
    "actions = rl_data_test['Action'].unique().tolist() + [\"No Action\"]\n",
    "\n",
    "# Map the action index to the action name\n",
    "if 0 <= action < len(actions):\n",
    "    action_name = actions[action]\n",
    "    print(f\"Predicted action name: {action_name}\")\n",
    "else:\n",
    "    print(\"Error: Action index out of bounds!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
