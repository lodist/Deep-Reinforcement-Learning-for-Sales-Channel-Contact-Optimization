{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENVIRONMENT SETUP \n",
    "\n",
    "class CustomerInteractionEnv(gym.Env):\n",
    "    def __init__(self, dataset, total_timesteps):\n",
    "        super(CustomerInteractionEnv, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.current_step = 0\n",
    "        self.total_reward = 0  # Track cumulative reward\n",
    "        self.total_timesteps = total_timesteps  # Total timesteps for training\n",
    "        self.actions = dataset['Action'].unique().tolist() + [\"No Action\"]\n",
    "        self.action_space = spaces.Discrete(len(self.actions))\n",
    "        state_size = len(dataset.iloc[0]['State'])\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(state_size,), dtype=np.float32)\n",
    "        self.visited_states = defaultdict(int)  # Track visited states\n",
    "        self.action_counts = defaultdict(int)  # Track action frequencies\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        \"\"\"Convert the state to a NumPy array.\"\"\"\n",
    "        return np.array(state, dtype=np.float32)\n",
    "\n",
    "    def calculate_similarity(self, action1, action2):\n",
    "        \"\"\"Custom similarity measure between actions.\"\"\"\n",
    "        return 1 if action1 == action2 else 0  # Replace with actual similarity logic\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset the environment to the first step.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        self.total_reward = 0  # Reset cumulative reward\n",
    "        self.visited_states.clear()  # Reset visit counts\n",
    "        self.action_counts.clear()  # Reset action frequencies\n",
    "        initial_state = self.preprocess_state(self.dataset.iloc[self.current_step]['State'])\n",
    "        return initial_state, {}  # Return a tuple (obs, info)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return the next state, reward, terminated, truncated, and info.\"\"\"\n",
    "        row = self.dataset.iloc[self.current_step]\n",
    "        chosen_action = self.actions[action]\n",
    "\n",
    "        # Increment action count\n",
    "        self.action_counts[chosen_action] += 1\n",
    "\n",
    "        if chosen_action == \"No Action\":\n",
    "            reward = -0.01  \n",
    "        elif chosen_action == row['Action']:\n",
    "            reward = row['Reward']  # Positive reward\n",
    "            \n",
    "            # Enhance positive rewards between 10 and 20\n",
    "            if 10 <= reward <= 20:\n",
    "                reward = reward ** 1.5\n",
    "        else:\n",
    "            similarity_score = self.calculate_similarity(chosen_action, row['Action'])\n",
    "            reward = similarity_score * 0.5 - 0.2\n",
    "\n",
    "        # Penalize repeated actions\n",
    "        action_penalty = 0.001 * (self.action_counts[chosen_action] - 1)\n",
    "        reward -= action_penalty\n",
    "\n",
    "        # Exploration bonus for visiting less-explored states\n",
    "        state_tuple = tuple(self.preprocess_state(row['State']))\n",
    "        self.visited_states[state_tuple] += 1\n",
    "        decay_factor = 1 - (self.current_step / self.total_timesteps)  # Linearly decay over time\n",
    "        decay_factor = max(decay_factor, 0)  # Ensure the decay factor never goes below 0\n",
    "        exploration_bonus = (0.8 / np.sqrt(self.visited_states[state_tuple])) * decay_factor\n",
    "        exploration_bonus = min(exploration_bonus, 1.0)  # Cap the bonus at 1.0\n",
    "        reward += exploration_bonus\n",
    "\n",
    "        # Update total reward\n",
    "        self.total_reward += reward\n",
    "\n",
    "        if self.current_step % 10_000 == 0:\n",
    "            print(\n",
    "                f\"Step: {self.current_step}, Action: {chosen_action}, Reward: {reward:.2f}, \"\n",
    "                f\"Exploration Bonus: {exploration_bonus:.2f}, Total Reward: {self.total_reward:.2f}\"\n",
    "            )\n",
    "\n",
    "        self.current_step += 1\n",
    "        terminated = self.current_step >= len(self.dataset)\n",
    "        truncated = False\n",
    "        if not terminated:\n",
    "            next_state = self.preprocess_state(self.dataset.iloc[self.current_step]['State'])\n",
    "        else:\n",
    "            next_state = None\n",
    "        return next_state, reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Render the environment (optional).\"\"\"\n",
    "        print(f\"Step: {self.current_step}, Total Steps: {len(self.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET PREPARATION \n",
    "\n",
    "# Load the RL training dataset\n",
    "rl_data = pd.read_csv(\"DRL_Training_Dataset_Subset.csv\")\n",
    "\n",
    "# Convert the State column to a list of floats\n",
    "rl_data['State'] = rl_data['State'].apply(eval)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENVIRONMENT INITIALIZATION \n",
    "\n",
    "# Split into train (80%) and test (20%)\n",
    "train_data, test_data = train_test_split(rl_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create separate environments\n",
    "env_train = CustomerInteractionEnv(dataset=train_data, total_timesteps=5_000_000)\n",
    "env_test = CustomerInteractionEnv(dataset=test_data, total_timesteps=500_000)  # For later evaluation\n",
    "\n",
    "# Check the custom environment\n",
    "check_env(env_train, warn=True)\n",
    "check_env(env_test, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a DQN model\n",
    "\n",
    "class ProgressBarCallback(BaseCallback):\n",
    "    def __init__(self, total_timesteps, verbose=0):\n",
    "        super(ProgressBarCallback, self).__init__(verbose)\n",
    "        self.total_timesteps = total_timesteps\n",
    "        self.last_time_called = time.time()  # Initialize the timer\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        current_time = time.time()\n",
    "        # Check if one minute has passed\n",
    "        if current_time - self.last_time_called >= 180:  # 180 seconds\n",
    "            self.last_time_called = current_time\n",
    "            # Calculate progress\n",
    "            current_progress = self.num_timesteps / self.total_timesteps * 100\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Training progress: {current_progress:.2f}%\")\n",
    "        return True\n",
    "\n",
    "\n",
    "# Hyperparameters hidden in a dictionary\n",
    "drl_hyperparameters = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"gamma\": 0.98,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"buffer_size\": 300_000,\n",
    "    \"learning_starts\": 5_000,\n",
    "    \"batch_size\": 128,\n",
    "    \"tau\": 0.01,\n",
    "    \"train_freq\": 4,\n",
    "    \"exploration_fraction\": 0.5,\n",
    "    \"exploration_final_eps\": 0.02,\n",
    "    \"target_update_interval\": 5_000,\n",
    "    \"max_grad_norm\": 5,\n",
    "    \"policy_kwargs\": {\"net_arch\": [512, 256]},\n",
    "    \"verbose\": 1,\n",
    "    \"total_timesteps\": 5_000_000,\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize the DQN model\n",
    "model = DQN(\n",
    "    drl_hyperparameters[\"policy\"],\n",
    "    env_train,\n",
    "    gamma=drl_hyperparameters[\"gamma\"],\n",
    "    learning_rate=drl_hyperparameters[\"learning_rate\"],\n",
    "    buffer_size=drl_hyperparameters[\"buffer_size\"],\n",
    "    learning_starts=drl_hyperparameters[\"learning_starts\"],\n",
    "    batch_size=drl_hyperparameters[\"batch_size\"],\n",
    "    tau=drl_hyperparameters[\"tau\"],\n",
    "    train_freq=drl_hyperparameters[\"train_freq\"],\n",
    "    exploration_fraction=drl_hyperparameters[\"exploration_fraction\"],\n",
    "    exploration_final_eps=drl_hyperparameters[\"exploration_final_eps\"],\n",
    "    target_update_interval=drl_hyperparameters[\"target_update_interval\"],\n",
    "    max_grad_norm=drl_hyperparameters[\"max_grad_norm\"],\n",
    "    policy_kwargs=drl_hyperparameters[\"policy_kwargs\"],\n",
    "    verbose=drl_hyperparameters[\"verbose\"],\n",
    ")\n",
    "\n",
    "# Train the model with a progress bar callback\n",
    "progress_bar = ProgressBarCallback(\n",
    "    total_timesteps=drl_hyperparameters[\"total_timesteps\"], verbose=1\n",
    ")\n",
    "model.learn(\n",
    "    total_timesteps=drl_hyperparameters[\"total_timesteps\"], callback=progress_bar\n",
    ")\n",
    "\n",
    "model.save(\"dqn_customer_interaction\")\n",
    "print(\"Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL EVALUATION\n",
    "\n",
    "# Load the model\n",
    "model = DQN.load(\"dqn_customer_interaction\")\n",
    "\n",
    "def evaluate_model_performance(env, model, dataset, max_steps=1000):\n",
    "    \"\"\"Evaluate model vs dataset: Assign reward only if actions match, otherwise reward = 0.\"\"\"\n",
    "    obs = env.reset()[0]\n",
    "    model_total_reward = 0\n",
    "    dataset_total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    model_path = []\n",
    "    dataset_path = []\n",
    "    \n",
    "    no_action_count = 0\n",
    "    dm_count = 0  \n",
    "    em_count = 0  \n",
    "    dataset_dm_count = 0  \n",
    "    dataset_em_count = 0  \n",
    "    model_negative_rewards = 0\n",
    "    model_positive_rewards = 0\n",
    "    dataset_negative_rewards = 0\n",
    "    dataset_positive_rewards = 0\n",
    "    no_action_wins = 0  # When avoiding an action prevents a negative reward\n",
    "    no_action_losses = 0  # When avoiding an action misses a positive reward\n",
    "\n",
    "    while steps < max_steps:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        model_action = env.actions[action]\n",
    "        dataset_action = dataset.iloc[steps]['Action']\n",
    "        dataset_reward = dataset.iloc[steps]['Reward']\n",
    "\n",
    "        # Assign reward only if action matches dataset\n",
    "        model_reward = dataset_reward if model_action == dataset_action else 0\n",
    "        model_total_reward += model_reward\n",
    "\n",
    "        # Track positive and negative rewards\n",
    "        if model_reward < 0:\n",
    "            model_negative_rewards += 1\n",
    "        elif model_reward > 0:\n",
    "            model_positive_rewards += 1\n",
    "\n",
    "        if dataset_reward < 0:\n",
    "            dataset_negative_rewards += 1\n",
    "        elif dataset_reward > 0:\n",
    "            dataset_positive_rewards += 1\n",
    "\n",
    "        # Check if \"No Action\" was beneficial or harmful\n",
    "        if model_action == \"No Action\" and dataset_action != \"No Action\":\n",
    "            if dataset_reward < 0:\n",
    "                no_action_wins += 1  # Model avoided a bad action\n",
    "            elif dataset_reward > 0:\n",
    "                no_action_losses += 1  # Model skipped a good action\n",
    "\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        model_path.append({\"step\": steps, \"state\": obs.tolist(), \"action\": model_action, \"reward\": model_reward})\n",
    "        dataset_path.append({\"step\": steps, \"state\": obs.tolist(), \"action\": dataset_action, \"reward\": dataset_reward})\n",
    "        dataset_total_reward += dataset_reward\n",
    "\n",
    "        # Count actions\n",
    "        if model_action == \"No Action\":\n",
    "            no_action_count += 1\n",
    "        elif model_action == \"DM_sent\":\n",
    "            dm_count += 1\n",
    "        elif model_action == \"EMsent\":\n",
    "            em_count += 1\n",
    "\n",
    "        if dataset_action == \"DM_sent\":\n",
    "            dataset_dm_count += 1\n",
    "        elif dataset_action == \"EMsent\":\n",
    "            dataset_em_count += 1\n",
    "\n",
    "        steps += 1\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    model_total_contacts = dm_count + em_count\n",
    "    dataset_total_contacts = dataset_dm_count + dataset_em_count\n",
    "\n",
    "    model_total_cost = dm_count * 1  \n",
    "    dataset_total_cost = dataset_dm_count * 1  \n",
    "\n",
    "    model_reward_per_contact = model_total_reward / model_total_contacts if model_total_contacts > 0 else 0\n",
    "    dataset_reward_per_contact = dataset_total_reward / dataset_total_contacts if dataset_total_contacts > 0 else 0\n",
    "\n",
    "    model_cost_per_reward = model_total_cost / model_total_reward if model_total_reward != 0 else float('inf')\n",
    "    dataset_cost_per_reward = dataset_total_cost / dataset_total_reward if dataset_total_reward != 0 else float('inf')\n",
    "\n",
    "    # Print Results\n",
    "    print(f\"Model Total Reward: {model_total_reward:.2f} (Only for matched actions)\")\n",
    "    print(f\"Dataset Total Reward: {dataset_total_reward:.2f}\")\n",
    "    print(f\"Total Contacts in Model: {model_total_contacts} out of {max_steps}\")\n",
    "    print(f\"Total Contacts in Dataset: {dataset_total_contacts} out of {max_steps}\")\n",
    "    print(f\"Difference in Contacts Between Model and Dataset: {model_total_contacts - dataset_total_contacts}\")\n",
    "    print(f\"Model Total Cost: {model_total_cost} CHF\")\n",
    "    print(f\"Dataset Total Cost: {dataset_total_cost} CHF\")\n",
    "    print(f\"Model Reward per Contact: {model_reward_per_contact:.4f}\")\n",
    "    print(f\"Dataset Reward per Contact: {dataset_reward_per_contact:.4f}\")\n",
    "    print(f\"Model Cost per Reward Point: {model_cost_per_reward:.4f} CHF\")\n",
    "    print(f\"Dataset Cost per Reward Point: {dataset_cost_per_reward:.4f} CHF\")\n",
    "    print(f\"Model Negative Reward Count: {model_negative_rewards} out of {max_steps}\")\n",
    "    print(f\"Dataset Negative Reward Count: {dataset_negative_rewards} out of {max_steps}\")\n",
    "    print(f\"'No Action' Wins (Avoided Bad Actions): {no_action_wins} times\")\n",
    "    print(f\"'No Action' Losses (Missed Good Actions): {no_action_losses} times\")\n",
    "\n",
    "    return model_path, dataset_path\n",
    "\n",
    "def plot_model_performance(model_path, dataset_path):\n",
    "    # Extract rewards and actions\n",
    "    model_rewards = [step[\"reward\"] for step in model_path]\n",
    "    dataset_rewards = [step[\"reward\"] for step in dataset_path]\n",
    "    model_actions = [step[\"action\"] for step in model_path]\n",
    "    dataset_actions = [step[\"action\"] for step in dataset_path]\n",
    "\n",
    "    # Cumulative rewards\n",
    "    model_cumulative_reward = np.cumsum(model_rewards)\n",
    "    dataset_cumulative_reward = np.cumsum(dataset_rewards)\n",
    "\n",
    "    # Cumulative cost (1 CHF per DM_sent)\n",
    "    model_cumulative_cost = np.cumsum([1 if action == \"DM_sent\" else 0 for action in model_actions])\n",
    "    dataset_cumulative_cost = np.cumsum([1 if action == \"DM_sent\" else 0 for action in dataset_actions])\n",
    "\n",
    "    # Plot cumulative rewards\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(model_cumulative_reward, label=\"Model Cumulative Reward\", color=\"#2f58d4\")\n",
    "    plt.plot(dataset_cumulative_reward, label=\"Dataset Cumulative Reward\", color=\"#ab9f7d\", linestyle=\"dashed\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    plt.title(\"Cumulative Reward Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot cumulative cost\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(model_cumulative_cost, label=\"Model Cumulative Cost\", color=\"#2f58d4\")\n",
    "    plt.plot(dataset_cumulative_cost, label=\"Dataset Cumulative Cost\", color=\"#ab9f7d\", linestyle=\"dashed\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Cumulative Cost (CHF)\")\n",
    "    plt.title(\"Cumulative Cost Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Action Distribution\n",
    "    model_action_counts = {action: model_actions.count(action) for action in set(model_actions)}\n",
    "    dataset_action_counts = {action: dataset_actions.count(action) for action in set(dataset_actions)}\n",
    "\n",
    "    actions = sorted(set(model_action_counts.keys()).union(dataset_action_counts.keys()))\n",
    "    model_counts = [model_action_counts.get(action, 0) for action in actions]\n",
    "    dataset_counts = [dataset_action_counts.get(action, 0) for action in actions]\n",
    "\n",
    "    x = np.arange(len(actions))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(x - 0.2, model_counts, width=0.4, label=\"Model Actions\", color=\"#2f58d4\")\n",
    "    plt.bar(x + 0.2, dataset_counts, width=0.4, label=\"Dataset Actions\", color=\"#ab9f7d\", alpha=0.7)\n",
    "    plt.xticks(x, actions, rotation=45)\n",
    "    plt.xlabel(\"Actions\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Action Distribution: Model vs Dataset\")\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Sample dataset for evaluation\n",
    "eval_subset = rl_data.sample(10000, random_state=42).reset_index(drop=True)\n",
    "eval_env = CustomerInteractionEnv(eval_subset, total_timesteps=10_000_000)\n",
    "\n",
    "# Run evaluation\n",
    "model_path, dataset_path = evaluate_model_performance(env_test, model, test_data, max_steps=1000)\n",
    "\n",
    "# Call function after evaluation\n",
    "plot_model_performance(model_path, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL APPLICATION TEST\n",
    "\n",
    "# Choose a random starting state from test_data\n",
    "i = random.randint(0, 1000)\n",
    "state = test_data['State'].iloc[i] \n",
    "\n",
    "if isinstance(state, str):\n",
    "    state = eval(state)\n",
    "\n",
    "obs = np.array(state, dtype=np.float32)  # Convert to NumPy array\n",
    "\n",
    "# Get the best predicted action for this state\n",
    "action, _ = model.predict(obs, deterministic=True)\n",
    "\n",
    "# Get action name\n",
    "actions = test_data['Action'].unique().tolist() + [\"No Action\"]\n",
    "action_name = actions[action] if 0 <= action < len(actions) else \"Error\"\n",
    "\n",
    "# Print the best action\n",
    "print(f\"Predicted best action for state {i}: {action_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
